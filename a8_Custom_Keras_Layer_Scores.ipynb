{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_yin1zTAaDO"
   },
   "source": [
    "# Assignment 8: Programming a neural network layer & Scores\n",
    "\n",
    "[Keras](https://keras.io) is a high-level deep-learning framework building on top of [TensorFlow](https://www.tensorflow.org). These frameworks follow the _symbol-to-symbol derivatives_ approach, i.e. automatically derive a computational graph to calculate derivatives. You just need to declare your inputs as TensorFlow variables and use TensorFlow operations on them to compute the forward pass.  \n",
    "\n",
    "## Task 8.1\n",
    "\n",
    "Work through the [Keras tutorial on custom layers](https://keras.io/guides/making_new_layers_and_models_via_subclassing) to learn how to create your own neural network layer.  \n",
    "Create a custom Keras layer that computes Gaussian basis functions, i.e. a layer that maps an input vector $\\mathbf x \\in \\mathbb R^n$ onto an output vector $\\mathbf y = f(\\mathbf x) \\in \\mathbb R^m$ as follows:\n",
    "\\begin{align}\n",
    "  f: \\mathbf x \\in \\mathbb R^n \\mapsto \\left[w_i \\exp\\left(-\\frac{\\|\\mathbf x - \\boldsymbol\\mu_i\\|^2}{\\sigma_i^2}\\right)\\right]_{i=1..m} \\in \\mathbb R^m\n",
    "\\end{align}\n",
    "\n",
    "Instead of projecting an input $\\mathbf x$ onto a weight vector $\\mathbf w$ as the standard neuron function $f(\\mathbf x) = \\sigma(\\mathbf w \\cdot \\mathbf x + b)$ does, the Gaussian basis function becomes active (with weight $w_i$) for all inputs $\\mathbf x$ close to a prototype $\\boldsymbol \\mu_i$. This activation quickly decays with increasing distance of $\\mathbf x$ to $\\boldsymbol \\mu_i$. The parameter $\\sigma_i$ controls the width of the Gaussian, i.e. the size of the active region.\n",
    "\n",
    "For efficient tensor-based operations you need to correctly _broadcast_ the tensors for the difference operation: TensorFlow will pass an input matrix of shape `(batch size, input dim)` for $\\mathbf X$, while you will have a matrix of centers $\\boldsymbol \\mu$ of shape `(input dim, #units)`. To correctly [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html) them together, you will need Keras' [`expand_dims()`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/expand_dims) function to extend $\\mathbf X$'s shape to `(batch size, input dim, 1)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6uWaMJIAaDQ"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class RBFLayer(keras.layers.Layer):\n",
    "    # TODO: Add initializer parameters to init\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        # TODO: Do something with the additional parameters\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "        # TODO: Initialize properties depending on `input_shape`\n",
    "\n",
    "    def call(self, X):\n",
    "        # TODO: Process input and return\n",
    "        return X\n",
    "\n",
    "\n",
    "class InitCentersRandom(keras.initializers.Initializer):\n",
    "    \"\"\" Initializer to initialize centers of RBF network from random samples of the data set.\"\"\"\n",
    "\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        # TODO: Generate and return initialization\n",
    "        pass\n",
    "\n",
    "\n",
    "### dummy example to compute diffs between data X and centers mu\n",
    "X = tf.ones((3, 5))  # input tensor X with batch dimension 3 and data dim N=5\n",
    "mu = tf.ones((5, 2))  # tensor mu with data dim N=5 and 2 units\n",
    "diffs = K.expand_dims(X) - mu  # diffs tensor: 3 x 5 x 2\n",
    "print(X.shape, mu.shape, diffs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8_5z3zFAaDS"
   },
   "source": [
    "## Task 8.2\n",
    "\n",
    "Compare the performance of such a Gaussian basis function layer with that of a standard [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer on the MNIST dataset.  \n",
    "Hint: Utilize existing tutorials on setting up your first MNIST MLP with Keras, e.g. https://keras.io/guides/training_with_built_in_methods.\n",
    "\n",
    "To achieve decent performance, you want to:\n",
    "- Initialize the centers $\\boldsymbol \\mu_i$ from random data samples $\\mathbf x$ (create a custom [initializer](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Initializer))\n",
    "- Initialize $\\sigma_i$ to the typical in-class distance between data points.  \n",
    "  Use [`scipy.spatial.distance_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html) to compute this statistics on a random selection of your input data.  \n",
    "  (Doing it on the full dataset will probably exhaust your memory.)\n",
    "- Initialize $w_i = 1$\n",
    "\n",
    "Questions:\n",
    "- How many parameters each of those networks have?\n",
    "- Which network trains faster / easier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-Square Error and Maximum Likelihood Estimation\n",
    "\n",
    "In this section, we'll explore the relationship between least-square error and maximum likelihood estimation under the assumption of Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data with Gaussian Noise\n",
    "\n",
    "Let's start by generating some data from a linear model with Gaussian noise:\n",
    "\n",
    "$$y = \\theta^T x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "We'll visualize how the noise affects our observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(theta, sigma, n_samples=100, x_range=(0, 10)):\n",
    "    \"\"\"Generate data from a linear model with Gaussian noise.\"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], n_samples)\n",
    "    noise = np.random.normal(0, sigma, n_samples)\n",
    "    y = theta * x + noise\n",
    "    return x, y\n",
    "\n",
    "# Parameters\n",
    "true_theta = 2.5  # True slope\n",
    "sigma_values = [0.5, 2.0, 5.0]  # Different noise levels\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Generate and plot data with different noise levels\n",
    "for i, sigma in enumerate(sigma_values):\n",
    "    x, y = generate_linear_data(true_theta, sigma)\n",
    "    \n",
    "    # Plot the data points\n",
    "    axes[i].scatter(x, y, alpha=0.6, label=f'Data points')\n",
    "    \n",
    "    # Plot the true line\n",
    "    axes[i].plot(x, true_theta * x, 'r-', label='True line')\n",
    "    \n",
    "    # Add labels and title\n",
    "    axes[i].set_xlabel('x')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].set_title(f'Gaussian Noise with σ = {sigma}')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "Now, let's visualize how the likelihood function changes as we vary the parameter $\\theta$. We'll see that the maximum likelihood estimate corresponds to the least-squares solution.\n",
    "### Task 8.3: Implement the log_likelihood function. How does the log_likelihood and squared error change with different thetas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, sigma):\n",
    "    \"\"\"Compute the log-likelihood for a linear model with Gaussian noise.\"\"\"\n",
    "    n = len(x)\n",
    "    # Log-likelihood formula: -n/2 * log(2πσ²) - 1/(2σ²) * Σ(y - θx)²\n",
    "    return #TODO\n",
    "\n",
    "def squared_error(theta, x, y):\n",
    "    \"\"\"Compute the sum of squared errors.\"\"\"\n",
    "    return np.sum((y - theta * x)**2)\n",
    "\n",
    "# Generate data\n",
    "x, y = generate_linear_data(true_theta, 2.0, n_samples=50)\n",
    "\n",
    "# Range of theta values to evaluate\n",
    "theta_range = np.linspace(0, 5, 100)\n",
    "\n",
    "# Compute log-likelihood and squared error for each theta\n",
    "log_likelihoods = [log_likelihood(theta, x, y, 2.0) for theta in theta_range]\n",
    "squared_errors = [squared_error(theta, x, y) for theta in theta_range]\n",
    "\n",
    "# Find the maximum likelihood estimate and minimum squared error\n",
    "mle_index = np.argmax(log_likelihoods)\n",
    "mse_index = np.argmin(squared_errors)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot log-likelihood\n",
    "ax1.plot(theta_range, log_likelihoods, 'b-')\n",
    "ax1.axvline(theta_range[mle_index], color='r', linestyle='--', \n",
    "           label=f'MLE: θ = {theta_range[mle_index]:.2f}')\n",
    "ax1.axvline(true_theta, color='g', linestyle='--', \n",
    "           label=f'True: θ = {true_theta:.2f}')\n",
    "ax1.set_xlabel('θ')\n",
    "ax1.set_ylabel('Log-likelihood')\n",
    "ax1.set_title('Log-likelihood Function')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot squared error\n",
    "ax2.plot(theta_range, squared_errors, 'b-')\n",
    "ax2.axvline(theta_range[mse_index], color='r', linestyle='--', \n",
    "           label=f'Min SSE: θ = {theta_range[mse_index]:.2f}')\n",
    "ax2.axvline(true_theta, color='g', linestyle='--', \n",
    "           label=f'True: θ = {true_theta:.2f}')\n",
    "ax2.set_xlabel('θ')\n",
    "ax2.set_ylabel('Sum of Squared Errors')\n",
    "ax2.set_title('Squared Error Function')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum Likelihood Estimate: θ = {theta_range[mle_index]:.4f}\")\n",
    "print(f\"Minimum Squared Error: θ = {theta_range[mse_index]:.4f}\")\n",
    "print(f\"True Parameter: θ = {true_theta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Demonstration\n",
    "\n",
    "Let's create an interactive demonstration to see how the maximum likelihood estimate changes with different noise levels and sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sigma=(0.5, 5.0, 0.5), n_samples=(10, 200, 10))\n",
    "def plot_mle_interactive(sigma, n_samples):\n",
    "    # Generate data\n",
    "    x, y = generate_linear_data(true_theta, sigma, n_samples=n_samples)\n",
    "    \n",
    "    # Compute the analytical MLE (which is the same as least squares for linear regression)\n",
    "    theta_mle = np.sum(x * y) / np.sum(x**2)\n",
    "    \n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(x, y, alpha=0.6, label='Data points')\n",
    "    \n",
    "    # Plot the true line\n",
    "    plt.plot(x, true_theta * x, 'r-', label='True line')\n",
    "    \n",
    "    # Plot the MLE line\n",
    "    plt.plot(x, theta_mle * x, 'g--', label='MLE line')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'MLE with σ = {sigma}, n = {n_samples}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Maximum Likelihood Estimate: θ = {theta_mle:.4f}\")\n",
    "    print(f\"True Parameter: θ = {true_theta:.4f}\")\n",
    "    print(f\"Difference: {abs(theta_mle - true_theta):.4f}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss and Classification\n",
    "\n",
    "In this section, we'll explore cross-entropy loss for classification problems, focusing on both binary and multi-class scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification with Sigmoid\n",
    "\n",
    "First, let's revisit visualize the sigmoid function and understand its role in binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Compute the sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Range of values\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = sigmoid(x)\n",
    "\n",
    "# Plot the sigmoid function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('σ(x)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.grid(True)\n",
    "plt.text(1, 0.45, 'σ(0) = 0.5', fontsize=12)\n",
    "plt.text(-8, 0.9, 'limₓ→∞ σ(x) = 1', fontsize=12)\n",
    "plt.text(-8, 0.1, 'limₓ→-∞ σ(x) = 0', fontsize=12)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross-Entropy Loss\n",
    "\n",
    "Now, let's visualize the binary cross-entropy loss function for different true labels and predicted probabilities.\n",
    "#### Task 8.4: Implement the binary cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"Compute the binary cross-entropy loss.\"\"\"\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return #TODO\n",
    "\n",
    "# Range of predicted probabilities\n",
    "y_pred = np.linspace(0.001, 0.999, 1000)\n",
    "\n",
    "# Compute loss for true label = 1 and true label = 0\n",
    "loss_y1 = binary_cross_entropy(1, y_pred)\n",
    "loss_y0 = binary_cross_entropy(0, y_pred)\n",
    "\n",
    "# Plot the loss functions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_pred, loss_y1, 'b-', linewidth=2, label='True label = 1')\n",
    "plt.plot(y_pred, loss_y0, 'r-', linewidth=2, label='True label = 0')\n",
    "plt.xlabel('Predicted Probability (ŷ)')\n",
    "plt.ylabel('Binary Cross-Entropy Loss')\n",
    "plt.title('Binary Cross-Entropy Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function for Multi-class Classification\n",
    "\n",
    "Let's visualize the softmax function for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function.\"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "@interact(a1=(-5.0, 5.0, 0.5), a2=(-5.0, 5.0, 0.5), a3=(-5.0, 5.0, 0.5))\n",
    "def plot_softmax_interactive(a1, a2, a3):\n",
    "    # Network outputs\n",
    "    a = np.array([a1, a2, a3])\n",
    "    \n",
    "    # Compute softmax probabilities\n",
    "    probs = softmax(a)\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['Class 1', 'Class 2', 'Class 3'], probs, color=['blue', 'green', 'red'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Softmax Probabilities')\n",
    "    \n",
    "    # Add text labels\n",
    "    for i, p in enumerate(probs):\n",
    "        plt.text(i, p + 0.02, f'{p:.4f}', ha='center')\n",
    "    \n",
    "    # Print the network outputs and softmax probabilities\n",
    "    print(f\"Network Outputs: [{a1:.2f}, {a2:.2f}, {a3:.2f}]\")\n",
    "    print(f\"Softmax Probabilities: [{probs[0]:.4f}, {probs[1]:.4f}, {probs[2]:.4f}]\")\n",
    "    print(f\"Sum of Probabilities: {np.sum(probs):.4f}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross-Entropy Loss\n",
    "\n",
    "Now, let's visualize the categorical cross-entropy loss for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"Compute the categorical cross-entropy loss.\"\"\"\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Example: True class is 2 (one-hot encoded)\n",
    "y_true = np.array([0, 1, 0])\n",
    "\n",
    "# Let's see how the loss changes as we vary the probability for the true class\n",
    "p_true_range = np.linspace(0.01, 0.99, 100)\n",
    "losses = []\n",
    "\n",
    "for p_true in p_true_range:\n",
    "    # Distribute the remaining probability equally among the other classes\n",
    "    p_others = (1 - p_true) / 2\n",
    "    y_pred = np.array([p_others, p_true, p_others])\n",
    "    losses.append(categorical_cross_entropy(y_true, y_pred))\n",
    "\n",
    "# Plot the loss function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_true_range, losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Probability for True Class')\n",
    "plt.ylabel('Categorical Cross-Entropy Loss')\n",
    "plt.title('Categorical Cross-Entropy Loss (True Class = 2)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Let's visualize the entropy of a binary distribution as we vary the probability of one outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"Compute the entropy of a binary distribution with probability p.\"\"\"\n",
    "    # Avoid log(0)\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
    "\n",
    "# Range of probabilities\n",
    "p_range = np.linspace(0.001, 0.999, 1000)\n",
    "entropies = [entropy(p) for p in p_range]\n",
    "\n",
    "# Plot the entropy function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_range, entropies, 'b-', linewidth=2)\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', alpha=0.3)\n",
    "plt.axhline(y=1, color='r', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Probability (p)')\n",
    "plt.ylabel('Entropy H(p)')\n",
    "plt.title('Entropy of a Binary Distribution')\n",
    "plt.grid(True)\n",
    "plt.text(0.55, 0.9, 'Maximum entropy at p = 0.5', fontsize=12)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy and KL Divergence\n",
    "\n",
    "Now, let's visualize the relationship between entropy, cross-entropy, and KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p, q):\n",
    "    \"\"\"Compute the cross-entropy between distributions p and q.\"\"\"\n",
    "    # Avoid log(0)\n",
    "    if q == 0 or q == 1:\n",
    "        return float('inf')\n",
    "    return -p * np.log2(q) - (1 - p) * np.log2(1 - q)\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute the KL divergence between distributions p and q.\"\"\"\n",
    "    return cross_entropy(p, q) - entropy(p)\n",
    "\n",
    "# Fix p and vary q\n",
    "p = 0.7\n",
    "q_range = np.linspace(0.001, 0.999, 1000)\n",
    "\n",
    "# Compute entropy, cross-entropy, and KL divergence\n",
    "h_p = entropy(p)\n",
    "h_p_q = [cross_entropy(p, q) for q in q_range]\n",
    "kl_p_q = [kl_divergence(p, q) for q in q_range]\n",
    "\n",
    "# Plot the functions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(q_range, h_p_q, 'b-', linewidth=2, label='Cross-Entropy H(p,q)')\n",
    "plt.plot(q_range, kl_p_q, 'g-', linewidth=2, label='KL Divergence D_KL(p||q)')\n",
    "plt.axhline(y=h_p, color='r', linestyle='--', linewidth=2, label='Entropy H(p)')\n",
    "plt.axvline(x=p, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('q')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'Entropy, Cross-Entropy, and KL Divergence (p = {p})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 3)\n",
    "plt.text(p + 0.02, 0.1, f'q = p = {p}', fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

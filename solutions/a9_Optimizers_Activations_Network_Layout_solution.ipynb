{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with different optimizers, network layouts, and activation functions\n",
    "\n",
    "Drawing on the simple MNIST classifier from task 6.2, evaluate different optimizers, network layouts, and activation functions.  \n",
    "To this end, first learn how to visualize the training history of a Keras model:\n",
    "* [Using the `History` of a training](https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras) (not detailed enough)\n",
    "* [Visualizing loss and metrics during training](https://www.tensorflow.org/guide/keras/training_with_built_in_methods#visualizing_loss_and_metrics_during_training)\n",
    "* [Using the `TensorBoard` callback in Keras](https://www.tensorflow.org/tensorboard/scalars_and_keras)\n",
    "\n",
    "To see the detailed evolution of errors, you should consider a resolution as fine-grained as your mini-batches.\n",
    "\n",
    "## Task 7.1\n",
    "\n",
    "Consider the simple network below (with a single hidden layer of 64 neurons) and compare the learning curves of networks with ReLu, logistic and tanh activations.\n",
    "What happens if you use different activation functions also in the output layer?\n",
    "\n",
    "## Task 7.2\n",
    "\n",
    "Going back to the original ReLu-based network, compare [various optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), i.e. SGD, RMSProp, Adam, ...\n",
    "How does their performance differ?\n",
    "\n",
    "## Task 7.3\n",
    "\n",
    "Compare various network layouts:\n",
    "* Try to adapt the number of hidden neurons, e.g. 32, 64, 128.\n",
    "* Try to add more hidden layers.\n",
    "* What is the best performance you can achieve (neglecting all regularization approaches for now)?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Load MNIST data set\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Neural Network with a single hidden layer\n",
    "from tensorflow.keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.Dense(10))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Training & Validation accuracies and losses\n",
    "The 2 ways we could do so are:\n",
    "\n",
    "- Manually plotting history values stored by Keras via matplotlib,\n",
    "- or automatically with the help of TensorBoard!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot model's training and validation loss/accuracy over time\n",
    "def plot_results(histories, path_to_save=None, model_name=None):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(25, 8))\n",
    "    for key, history in histories.items():\n",
    "      # summarize history for accuracy\n",
    "      axs[0].plot(history['sparse_categorical_accuracy'], label=key + ' train')\n",
    "      axs[0].plot(history['val_sparse_categorical_accuracy'], label=key + ' val')\n",
    "      axs[0].set_title('Model Accuracy')\n",
    "      axs[0].set_xlabel('Epoch')\n",
    "      axs[0].set_ylabel('Accuracy')\n",
    "      axs[0].legend()\n",
    "\n",
    "      # summarize history for loss\n",
    "      axs[1].plot(history['loss'], label=key + ' train')\n",
    "      axs[1].plot(history['val_loss'], label=key + ' val')\n",
    "      axs[1].set_title('Model Loss')\n",
    "      axs[1].set_xlabel('Epoch')\n",
    "      axs[1].set_ylabel('Loss')\n",
    "      axs[1].legend()\n",
    "\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train_model(model, label, optimizer=keras.optimizers.legacy.RMSprop(), epochs=10):\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  # Loss function to minimize\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  # List of metrics to monitor (additionally to loss)\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    # Create TensorBoard callback writing its data into label-specific log dir\n",
    "    tb = keras.callbacks.TensorBoard(log_dir=log_dir + label, update_freq='batch')\n",
    "    # Fit data to the model!\n",
    "    history = model.fit(x_train, y_train, batch_size=100, epochs=epochs,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[tb])  # callbacks are called after each batch and epoch\n",
    "\n",
    "    # return Keras history as dictionary indexed by label\n",
    "    return {label: history.history}\n",
    "\n",
    "results = dict() # initialize empty results dict"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "log_dir = 'logs/activations/'\n",
    "\n",
    "for activation in ['relu', 'tanh', 'sigmoid']:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation=activation, input_shape=(28 * 28,)))\n",
    "    model.add(layers.Dense(10))\n",
    "    results.update(train_model(model, label=activation))\n",
    "\n",
    "for activation in ['relu', 'tanh', 'sigmoid']:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation=activation, input_shape=(28 * 28,)))\n",
    "    model.add(layers.Dense(10, activation=activation))\n",
    "    results.update(train_model(model, label=activation + ' out'))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the results\n",
    "plot_results(results)\n",
    "%tensorboard --logdir logs/activations"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "log_dir = 'logs/optimizers/'\n",
    "\n",
    "from tensorflow.keras.optimizers import Adadelta, Adagrad, Adam, RMSprop, SGD\n",
    "\n",
    "results=dict()\n",
    "for opt in [Adadelta(), Adagrad(), Adam(), RMSprop(), SGD()]:\n",
    "    label = type(opt).__name__\n",
    "    print(label)\n",
    "    # recreate model to have a fresh set of random weights\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(28 * 28,)))\n",
    "    model.add(layers.Dense(10))\n",
    "    results.update(train_model(model, label=label, optimizer=opt, epochs=20))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the results\n",
    "plot_results(results)\n",
    "%tensorboard --logdir logs/optimizers"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "log_dir = 'logs/nets/'\n",
    "\n",
    "results=dict()\n",
    "for layer in [[32], [64], [128], [256], [64, 64], [128, 64], [64, 64, 64]]:\n",
    "    label = ' - '.join([str(n) for n in layer])\n",
    "    print(label)\n",
    "    model = models.Sequential()\n",
    "    for num in layer:\n",
    "        model.add(layers.Dense(num, activation='relu', input_shape=(28 * 28,)))\n",
    "    model.add(layers.Dense(10))\n",
    "    results.update(train_model(model, label=label, epochs=20))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show the results\n",
    "plot_results(results)\n",
    "%tensorboard --logdir logs/nets"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 2,
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
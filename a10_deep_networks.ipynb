{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks: Visualizations and Explanations \n",
    "\n",
    "This notebook provides visualizations and explanations for key concepts related to training deep neural networks, complementing the lecture slides. We will explore:\n",
    "\n",
    "1.  **Adaptive Learning Rates:** Visualizing how different optimization algorithms navigate a loss landscape.\n",
    "2.  **Error Landscapes:** Understanding saddle points and their impact on optimization.\n",
    "3.  **Debugging:** Interpreting loss and accuracy curves during training.\n",
    "4.  **Performance Metrics:** Evaluating model performance, especially on imbalanced datasets.\n",
    "\n",
    "**Instructions:**\n",
    "- Complete all tasks marked with **TODO** comments\n",
    "- Answer all questions in the designated cells\n",
    "- Run all cells to verify your implementations\n",
    "- The exercises should take approximately 30 minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adaptive Learning Rates and Optimization Algorithms\n",
    "\n",
    "Gradient descent algorithms are used to minimize the loss function by iteratively updating the model parameters. Different algorithms adapt the learning rate or use momentum to navigate the loss landscape more effectively. Let's visualize the paths taken by different optimizers on a simple quadratic loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple quadratic loss function (representing a simple error landscape)\n",
    "def loss_function(w1, w2):\n",
    "    return 0.5 * (w1**2 + 10 * w2**2) # Elliptical bowl shape\n",
    "\n",
    "# Define gradients for the loss function\n",
    "def gradients(w1, w2):\n",
    "    return np.array([w1, 10 * w2])\n",
    "\n",
    "# --- Optimization Algorithms Implementation ---\n",
    "\n",
    "# Standard SGD\n",
    "def sgd(w_init, n_iterations, learning_rate):\n",
    "    w_path = [w_init]\n",
    "    w = w_init.copy()\n",
    "    for _ in range(n_iterations):\n",
    "        grad = gradients(w[0], w[1])\n",
    "        w = w - learning_rate * grad\n",
    "        w_path.append(w.copy())\n",
    "    return np.array(w_path)\n",
    "\n",
    "# TODO: Implement SGD with Momentum\n",
    "# The momentum update rule is: v = momentum * v + learning_rate * gradient, w = w - v\n",
    "def sgd_momentum(w_init, n_iterations, learning_rate, momentum=0.9):\n",
    "    w_path = [w_init]\n",
    "    w = w_init.copy()\n",
    "    v = np.zeros_like(w)\n",
    "    for _ in range(n_iterations):\n",
    "        # TODO: Implement momentum update\n",
    "        # 1. Calculate gradient\n",
    "        # 2. Update velocity v using momentum formula\n",
    "        # 3. Update weights w\n",
    "        # 4. Append to w_path\n",
    "        pass\n",
    "    return np.array(w_path)\n",
    "\n",
    "# RMSProp\n",
    "def rmsprop(w_init, n_iterations, learning_rate, decay_rate=0.9, epsilon=1e-8):\n",
    "    w_path = [w_init]\n",
    "    w = w_init.copy()\n",
    "    r = np.zeros_like(w) # Accumulated squared gradient\n",
    "    for _ in range(n_iterations):\n",
    "        grad = gradients(w[0], w[1])\n",
    "        r = decay_rate * r + (1 - decay_rate) * grad**2\n",
    "        update = (learning_rate / (np.sqrt(r) + epsilon)) * grad\n",
    "        w = w - update\n",
    "        w_path.append(w.copy())\n",
    "    return np.array(w_path)\n",
    "\n",
    "# Adam\n",
    "def adam(w_init, n_iterations, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    w_path = [w_init]\n",
    "    w = w_init.copy()\n",
    "    s = np.zeros_like(w) # First moment estimate\n",
    "    r = np.zeros_like(w) # Second moment estimate\n",
    "    t = 0\n",
    "    for _ in range(n_iterations):\n",
    "        t += 1\n",
    "        grad = gradients(w[0], w[1])\n",
    "        s = beta1 * s + (1 - beta1) * grad\n",
    "        r = beta2 * r + (1 - beta2) * grad**2\n",
    "        s_hat = s / (1 - beta1**t)\n",
    "        r_hat = r / (1 - beta2**t)\n",
    "        update = (learning_rate / (np.sqrt(r_hat) + epsilon)) * s_hat\n",
    "        w = w - update\n",
    "        w_path.append(w.copy())\n",
    "    return np.array(w_path)\n",
    "\n",
    "# --- Visualization ---\n",
    "\n",
    "# Initial point\n",
    "w_init = np.array([4.0, 1.5])\n",
    "n_iterations = 50\n",
    "\n",
    "# Run optimizers\n",
    "path_sgd_fast = sgd(w_init, n_iterations, learning_rate=0.18) # Can oscillate\n",
    "path_sgd_slow = sgd(w_init, n_iterations, learning_rate=0.05) # Slow convergence\n",
    "# TODO: Uncomment after implementing sgd_momentum\n",
    "# path_momentum = sgd_momentum(w_init, n_iterations, learning_rate=0.05, momentum=0.9)\n",
    "path_rmsprop = rmsprop(w_init, n_iterations, learning_rate=0.1)\n",
    "path_adam = adam(w_init, n_iterations, learning_rate=0.2)\n",
    "\n",
    "# Create contour plot\n",
    "w1_range = np.linspace(-5, 5, 100)\n",
    "w2_range = np.linspace(-2, 2, 100)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "Z = loss_function(W1, W2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "contour = plt.contour(W1, W2, Z, levels=np.logspace(-1, 3, 10), cmap='viridis')\n",
    "plt.colorbar(contour, label='Loss Value')\n",
    "\n",
    "# Plot paths\n",
    "plt.plot(path_sgd_fast[:, 0], path_sgd_fast[:, 1], 'o-', label='SGD (LR=0.18)', markersize=3, alpha=0.7)\n",
    "plt.plot(path_sgd_slow[:, 0], path_sgd_slow[:, 1], 's-', label='SGD (LR=0.05)', markersize=3, alpha=0.7)\n",
    "# TODO: Uncomment after implementing sgd_momentum\n",
    "# plt.plot(path_momentum[:, 0], path_momentum[:, 1], '^-', label='Momentum', markersize=3, alpha=0.7)\n",
    "plt.plot(path_rmsprop[:, 0], path_rmsprop[:, 1], 'd-', label='RMSProp', markersize=3, alpha=0.7)\n",
    "plt.plot(path_adam[:, 0], path_adam[:, 1], '*-', label='Adam', markersize=4, alpha=0.7)\n",
    "\n",
    "plt.plot(w_init[0], w_init[1], 'ko', label='Start') # Start point\n",
    "plt.plot(0, 0, 'rx', label='Minimum', markersize=10) # Minimum point\n",
    "\n",
    "plt.title('Optimization Paths on a Quadratic Loss Surface')\n",
    "plt.xlabel('w1')\n",
    "plt.ylabel('w2')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** After implementing the momentum algorithm and running the visualization, answer the following questions:\n",
    "\n",
    "1. How does momentum help with optimization compared to standard SGD?\n",
    "2. Why does SGD with a high learning rate oscillate more in the w2 direction than in the w1 direction?\n",
    "3. Which algorithm would you choose for a real-world deep learning problem and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers to Task 1:**\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Landscapes: Saddle Points\n",
    "\n",
    "In high-dimensional spaces, saddle points are much more common than local minima. A saddle point is a point where the gradient is zero, but it's a minimum along some dimensions and a maximum along others. Optimization algorithms can slow down significantly near saddle points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a saddle point function\n",
    "def saddle_function(w1, w2):\n",
    "    return w1**2 - w2**2\n",
    "\n",
    "# TODO: Implement the gradient function for the saddle function\n",
    "def saddle_gradients(w1, w2):\n",
    "    # The gradient of f(w1,w2) = w1^2 - w2^2 is [∂f/∂w1, ∂f/∂w2]\n",
    "    # TODO: Calculate and return the gradient as a numpy array\n",
    "    return np.array([0, 0])  # Replace with correct gradient\n",
    "\n",
    "# --- Optimization Algorithms (modified for saddle function) ---\n",
    "def sgd_saddle(w_init, n_iterations, learning_rate):\n",
    "    w_path = [w_init]\n",
    "    w = w_init.copy()\n",
    "    for _ in range(n_iterations):\n",
    "        grad = saddle_gradients(w[0], w[1])\n",
    "        w = w - learning_rate * grad\n",
    "        w_path.append(w.copy())\n",
    "    return np.array(w_path)\n",
    "\n",
    "def sgd_momentum_saddle(w_init, n_iterations, learning_rate, momentum=0.9):\n",
    "    w_path = [w_init]\n",
    "    w = w_init.copy()\n",
    "    v = np.zeros_like(w)\n",
    "    for _ in range(n_iterations):\n",
    "        grad = saddle_gradients(w[0], w[1])\n",
    "        v = momentum * v + learning_rate * grad\n",
    "        w = w - v\n",
    "        w_path.append(w.copy())\n",
    "    return np.array(w_path)\n",
    "\n",
    "# --- Visualization ---\n",
    "w_init_saddle = np.array([0.1, 1.5]) # Start near the saddle point, off the unstable axis\n",
    "n_iterations_saddle = 100\n",
    "\n",
    "path_sgd_saddle = sgd_saddle(w_init_saddle, n_iterations_saddle, learning_rate=0.1)\n",
    "path_momentum_saddle = sgd_momentum_saddle(w_init_saddle, n_iterations_saddle, learning_rate=0.1, momentum=0.7) # Lower momentum helps visualize escape\n",
    "\n",
    "# Create contour plot for saddle function\n",
    "w1_saddle_range = np.linspace(-2, 2, 100)\n",
    "w2_saddle_range = np.linspace(-2, 2, 100)\n",
    "W1_saddle, W2_saddle = np.meshgrid(w1_saddle_range, w2_saddle_range)\n",
    "Z_saddle = saddle_function(W1_saddle, W2_saddle)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "contour_saddle = plt.contour(W1_saddle, W2_saddle, Z_saddle, levels=20, cmap='coolwarm')\n",
    "plt.colorbar(contour_saddle, label='Function Value')\n",
    "\n",
    "# Plot paths\n",
    "plt.plot(path_sgd_saddle[:, 0], path_sgd_saddle[:, 1], 'o-', label='SGD', markersize=3, alpha=0.7)\n",
    "plt.plot(path_momentum_saddle[:, 0], path_momentum_saddle[:, 1], '^-', label='Momentum', markersize=3, alpha=0.7)\n",
    "\n",
    "plt.plot(w_init_saddle[0], w_init_saddle[1], 'ko', label='Start') # Start point\n",
    "plt.plot(0, 0, 'rx', label='Saddle Point', markersize=10) # Saddle point at (0,0)\n",
    "\n",
    "plt.title('Optimization Paths near a Saddle Point')\n",
    "plt.xlabel('w1 (minimum direction)')\n",
    "plt.ylabel('w2 (maximum direction)')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** After implementing the saddle point gradient function and observing the visualization, explain:\n",
    "\n",
    "1. Why are saddle points more common than local minima in high-dimensional spaces?\n",
    "2. How does momentum help with escaping saddle points?\n",
    "3. What would happen if we initialized both optimizers exactly at the saddle point (0,0)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers to Task 2:**\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Debugging: Interpreting Loss and Accuracy Curves\n",
    "\n",
    "Monitoring loss and accuracy curves during training is crucial for debugging and understanding model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some training curves\n",
    "epochs = np.arange(1, 101)\n",
    "\n",
    "# Scenario 1: Good convergence\n",
    "loss_good = 1.0 / epochs**0.5 + np.random.rand(100) * 0.1\n",
    "acc_train_good = 0.95 - 0.4 / epochs**0.5 + np.random.rand(100) * 0.02\n",
    "acc_val_good = 0.90 - 0.3 / epochs**0.5 + np.random.rand(100) * 0.02\n",
    "\n",
    "# Scenario 2: Overfitting\n",
    "loss_overfit = 0.5 / epochs**0.7 + np.random.rand(100) * 0.05\n",
    "acc_train_overfit = 0.98 - 0.2 / epochs**0.8 + np.random.rand(100) * 0.01\n",
    "acc_val_overfit = 0.75 + 0.1 * np.sin(epochs / 20) + np.random.rand(100) * 0.03 # Validation accuracy stagnates/decreases\n",
    "\n",
    "# TODO: Create a scenario for underfitting\n",
    "# Scenario 4: Underfitting - implement your own curves that would represent underfitting\n",
    "# Hint: Consider what happens when the model is too simple or training is insufficient\n",
    "loss_underfit = None  # TODO: Create appropriate loss curve\n",
    "acc_train_underfit = None  # TODO: Create appropriate training accuracy curve\n",
    "acc_val_underfit = None  # TODO: Create appropriate validation accuracy curve\n",
    "\n",
    "# Scenario 3: Learning rate too high (oscillating loss)\n",
    "loss_high_lr = 0.5 + np.abs(0.5 * np.sin(epochs / 5) + np.random.rand(100) * 0.2) + 1.0 / epochs\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 2, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Good Convergence\n",
    "axs[0, 0].plot(epochs, loss_good, label='Training Loss')\n",
    "axs[0, 0].plot(epochs, acc_train_good, label='Training Accuracy')\n",
    "axs[0, 0].plot(epochs, acc_val_good, label='Validation Accuracy')\n",
    "axs[0, 0].set_title('Good Convergence')\n",
    "axs[0, 0].set_xlabel('Epochs')\n",
    "axs[0, 0].set_ylabel('Value')\n",
    "axs[0, 0].legend()\n",
    "axs[0, 0].grid(True)\n",
    "\n",
    "# Plot 2: Overfitting\n",
    "axs[0, 1].plot(epochs, loss_overfit, label='Training Loss')\n",
    "axs[0, 1].plot(epochs, acc_train_overfit, label='Training Accuracy')\n",
    "axs[0, 1].plot(epochs, acc_val_overfit, label='Validation Accuracy')\n",
    "axs[0, 1].set_title('Overfitting')\n",
    "axs[0, 1].set_xlabel('Epochs')\n",
    "axs[0, 1].set_ylabel('Value')\n",
    "axs[0, 1].legend()\n",
    "axs[0, 1].grid(True)\n",
    "\n",
    "# Plot 3: High Learning Rate\n",
    "axs[1, 0].plot(epochs, loss_high_lr, label='Training Loss')\n",
    "axs[1, 0].set_title('High Learning Rate (Oscillating Loss)')\n",
    "axs[1, 0].set_xlabel('Epochs')\n",
    "axs[1, 0].set_ylabel('Loss')\n",
    "axs[1, 0].legend()\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "# TODO: Uncomment after implementing underfitting scenario\n",
    "# Plot 4: Underfitting\n",
    "# axs[1, 1].plot(epochs, loss_underfit, label='Training Loss')\n",
    "# axs[1, 1].plot(epochs, acc_train_underfit, label='Training Accuracy')\n",
    "# axs[1, 1].plot(epochs, acc_val_underfit, label='Validation Accuracy')\n",
    "# axs[1, 1].set_title('Underfitting')\n",
    "# axs[1, 1].set_xlabel('Epochs')\n",
    "# axs[1, 1].set_ylabel('Value')\n",
    "# axs[1, 1].legend()\n",
    "# axs[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** After implementing the underfitting scenario and observing all four plots, complete the following table by listing the key characteristics of each scenario and suggesting at least one solution for each problem:\n",
    "\n",
    "| Scenario | Key Characteristics | Potential Solutions |\n",
    "|----------|---------------------|---------------------|\n",
    "| Good Convergence | | |\n",
    "| Overfitting | | |\n",
    "| High Learning Rate | | |\n",
    "| Underfitting | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers to Task 3:**\n",
    "\n",
    "| Scenario | Key Characteristics | Potential Solutions |\n",
    "|----------|---------------------|---------------------|\n",
    "| Good Convergence | | |\n",
    "| Overfitting | | |\n",
    "| High Learning Rate | | |\n",
    "| Underfitting | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Metrics for Imbalanced Datasets\n",
    "\n",
    "Accuracy can be misleading on imbalanced datasets. Metrics like precision, recall, F1-score, and the ROC curve provide a more nuanced view of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                           n_informative=2, n_redundant=10, \n",
    "                           n_clusters_per_class=1, weights=[0.95, 0.05], \n",
    "                           flip_y=0, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Train a simple classifier\n",
    "model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1] # Probabilities for ROC curve\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Dataset Class Distribution (Test Set): {np.bincount(y_test)}\")\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# TODO: Calculate specificity (true negative rate)\n",
    "# Specificity = TN / (TN + FP)\n",
    "# Extract values from confusion matrix: cm = [[TN, FP], [FN, TP]]\n",
    "specificity = None  # TODO: Calculate specificity\n",
    "print(f'Specificity: {specificity:.4f}')\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** After calculating specificity and examining all the metrics, answer the following questions:\n",
    "\n",
    "1. Why is accuracy alone insufficient for evaluating models on imbalanced datasets?\n",
    "2. In a medical diagnosis scenario where the positive class represents a rare disease, which metric would be most important to optimize and why?\n",
    "3. What does the area under the ROC curve (AUC) represent, and why is it useful for imbalanced datasets?\n",
    "4. Suggest two techniques that could improve the model's performance on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers to Task 4:**\n",
    "\n",
    "1. \n",
    "\n",
    "2. \n",
    "\n",
    "3. \n",
    "\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook demonstrated key aspects of training and evaluating deep neural networks:\n",
    "\n",
    "*   Adaptive optimization algorithms like Adam and RMSProp often converge faster and more reliably than standard SGD, especially on complex loss surfaces.\n",
    "*   Understanding the behavior of optimizers near saddle points is important, as these are common in high-dimensional spaces.\n",
    "*   Monitoring training curves (loss, accuracy) is essential for debugging issues like overfitting or poor learning rates.\n",
    "*   For imbalanced datasets, metrics beyond accuracy (precision, recall, F1, AUC) are crucial for a complete performance evaluation.\n",
    "\n",
    "**Final Task:** Summarize the most important insight you gained from each of the four sections of this notebook and how you might apply it in your own deep learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your final summary:**\n",
    "\n",
    "1. Adaptive Learning Rates: \n",
    "\n",
    "2. Error Landscapes: \n",
    "\n",
    "3. Debugging: \n",
    "\n",
    "4. Performance Metrics: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook has been created with the help of LLMs, 17.06.2025._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
